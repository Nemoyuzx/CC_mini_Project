{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee69ba0",
   "metadata": {},
   "source": [
    "# Keras Text-to-Video Diffusion System\n",
    "This notebook implements a modular text-to-video generation pipeline using TensorFlow/Keras with optional components such as a latent video autoencoder and a diffusion-based UNet conditioned on text embeddings. The code targets NVIDIA A100 GPUs and leverages mixed precision for performance.\n",
    "\n",
    "**Project goals**\n",
    "- Reproducible setup for large-scale generative video models\n",
    "- Dataset pipeline for paired text-video clips\n",
    "- Optional latent-space autoencoder for video compression\n",
    "- Diffusion model with temporal attention conditioning on text\n",
    "- Training loops with TensorBoard logging and checkpointing\n",
    "- Sampling utilities for generating and visualizing videos\n",
    "- Evaluation stubs (loss curves, qualitative samples, FVD placeholder)\n",
    "- Ablation hooks and optional Gradio UI for interactive demos\n",
    "\n",
    "> **Dependencies**: TensorFlow (>=2.14 with CUDA 12.x support), Transformers, TensorBoard, Matplotlib, Seaborn, Pandas, ImageIO, MoviePy (optional for MP4), Gradio.\n",
    "\n",
    "> **GPU assumptions**: Designed for NVIDIA A100 40GB (or larger). Adjust batch sizes, frame counts, and resolution if VRAM differs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5962b",
   "metadata": {},
   "source": [
    "## Hyperparameter Reference\n",
    "| Component | Setting | Notes |\n",
    "|-----------|---------|-------|\n",
    "| Frame resolution | 64×64 (Stage 1), 128×128 (Stage 2) | Increase gradually as capacity allows |\n",
    "| Frames per clip | 16 (Stage 1), 32 (Stage 2) | Extendable with temporal attention |\n",
    "| Latent dimension | 512 | Used by optional VAE |\n",
    "| Diffusion steps | 1000 | Cosine beta schedule |\n",
    "| Batch size | 8 @64×64, 4 @128×128 | Assumes A100 40GB |\n",
    "| Base learning rate | 1e-4 (diffusion), 5e-4 (VAE) | AdamW with cosine decay + warmup |\n",
    "| Text encoder | CLIP ViT-B/32 (Transformers) | Swap for domain-specific models |\n",
    "| Mixed precision | float16 compute / float32 vars | Requires TF mixed precision |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a019f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended: CUDA >= 12.2, cuDNN >= 9, TensorFlow >= 2.14 compiled for CUDA 12.x / 推荐环境配置\n",
      "Active device(s): ['/physical_device:GPU:0']\n",
      "TensorFlow version: 2.17.0\n",
      "Mixed precision policy: <DTypePolicy \"mixed_float16\">\n",
      "Assuming >=40GB VRAM (NVIDIA A100). Adjust batch sizes for smaller GPUs. / 假设显存≥40GB，如显存不足请调小 batch size\n"
     ]
    }
   ],
   "source": [
    "# Environment setup, reproducibility, and device diagnostics / 环境设置、结果可复现性与设备诊断\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import pathlib\n",
    "from collections.abc import Iterable\n",
    "from typing import Any, Dict, List, Optional, Tuple, cast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import mixed_precision\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "import imageio\n",
    "import moviepy.editor as mpy\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Optional: comment out if not using Gradio UI / 可选：若不需要 Gradio UI 可注释掉\n",
    "try:\n",
    "    import gradio as gr\n",
    "    GRADIO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GRADIO_AVAILABLE = False\n",
    "\n",
    "# Expected CUDA/TensorFlow versions for NVIDIA A100 / NVIDIA A100 推荐的 CUDA 与 TensorFlow 版本\n",
    "print('Recommended: CUDA >= 12.2, cuDNN >= 9, TensorFlow >= 2.14 compiled for CUDA 12.x / 推荐环境配置')\n",
    "# Verify GPU availability / 检查 GPU 是否可用\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        DEVICE = gpus[0].name\n",
    "    except RuntimeError as err:\n",
    "        print(f'Failed to set memory growth: {err}')\n",
    "else:\n",
    "    DEVICE = 'CPU'\n",
    "\n",
    "print(f'Active device(s): {[gpu.name for gpu in gpus] if gpus else DEVICE}')\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "\n",
    "# Mixed precision policy tailored for A100 float16 Tensor Cores / 针对 A100 Tensor Core 的混合精度策略\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(f'Mixed precision policy: {mixed_precision.global_policy()}')\n",
    "\n",
    "# Global seeds for reproducibility / 设定全局随机种子确保结果可复现\n",
    "SEED = 2025\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Memory/VRAM assumptions / 默认显存假设\n",
    "print('Assuming >=40GB VRAM (NVIDIA A100). Adjust batch sizes for smaller GPUs. / 假设显存≥40GB，如显存不足请调小 batch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary for experiment tracking / 实验配置字典\n",
    "CONFIG: Dict[str, Any] = {\n",
    "    'experiment_name': 'text_to_video_diffusion',\n",
    "    'output_dir': './outputs',\n",
    "    'log_dir': './logs',\n",
    "    'dataset': {\n",
    "        'video_root': '/path/to/video/clips',  # TODO: set dataset path / 需手动填写数据集视频根路径\n",
    "        'metadata_csv': '/path/to/metadata.csv',  # contains columns [video_path, text, num_frames] / 包含 video_path、text、num_frames 等字段\n",
    "        'frames': 16,\n",
    "        'resolution': (64, 64),\n",
    "        'frame_step': 1,\n",
    "        'train_split': 0.9,\n",
    "    },\n",
    "    'model': {\n",
    "        'vae_latent_dim': 512,\n",
    "        'diffusion_steps': 1000,\n",
    "        'max_time_embeddings': 1024,\n",
    "        'channels': 3,\n",
    "        'base_channels': 128,\n",
    "        'channel_multipliers': [1, 2, 4],\n",
    "        'attention_resolutions': [4, 8],\n",
    "        'num_heads': 8,\n",
    "    },\n",
    "    'training': {\n",
    "        'vae_epochs': 30,\n",
    "        'diffusion_epochs_stage1': 100,\n",
    "        'diffusion_epochs_stage2': 50,\n",
    "        'batch_size_stage1': 8,\n",
    "        'batch_size_stage2': 4,\n",
    "        'learning_rate_diffusion': 1e-4,\n",
    "        'learning_rate_vae': 5e-4,\n",
    "        'weight_decay': 1e-4,\n",
    "        'warmup_steps': 2000,\n",
    "        'gradient_clip_norm': 1.0,\n",
    "        'ema_decay': 0.999,\n",
    "        'checkpoint_interval': 5,\n",
    "    },\n",
    "    'generation': {\n",
    "        'num_inference_steps': 50,\n",
    "        'guidance_scale': 7.5,\n",
    "        'default_prompts': [\n",
    "            'A serene waterfall cascading into a crystal clear pool at sunset.',\n",
    "            'A futuristic city skyline illuminated by neon lights during a rainy night.'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "pathlib.Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(CONFIG['log_dir']).mkdir(parents=True, exist_ok=True)\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbaa7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenizer and encoder utilities / 文本分词与编码工具\n",
    "class TextEncoder:\n",
    "    \"\"\"Wrapper for pretrained Transformer-based text encoder. / 预训练 Transformer 文本编码器封装\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'openai/clip-vit-base-patch32'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = TFAutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = int(self.model.config.hidden_size)\n",
    "\n",
    "    def __call__(self, texts: List[str]) -> tf.Tensor:\n",
    "        tokens = self.tokenizer(\n",
    "            texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        outputs = self.model(**tokens, training=False)\n",
    "        # Use pooled output or mean pooling depending on encoder / 根据编码器类型选择池化方式\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            embeddings = outputs.pooler_output\n",
    "        else:\n",
    "            embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
    "        embeddings = tf.math.l2_normalize(embeddings, axis=-1)\n",
    "        return tf.cast(embeddings, dtype=tf.float32)\n",
    "\n",
    "TEXT_ENCODER = TextEncoder()\n",
    "print(f'Text encoder loaded: hidden_dim={TEXT_ENCODER.embedding_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset utilities / 数据集工具函数\n",
    "def load_video_clip(path: str, num_frames: int, resolution: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"Loads a video file and returns normalized frames in [-1, 1]. Placeholder requires implementation. / 加载视频文件并返回归一化到 [-1, 1] 的帧，需按实际情况完善\"\"\"\n",
    "    with imageio.get_reader(path) as reader:\n",
    "        iter_method = getattr(reader, 'iter_data', None)\n",
    "        frames = []\n",
    "        if callable(iter_method):\n",
    "            frames_source = iter_method()\n",
    "            if not isinstance(frames_source, Iterable):\n",
    "                raise TypeError('iter_data must return an iterable of frames')\n",
    "            frames_iter = iter(cast(Iterable[np.ndarray], frames_source))\n",
    "            for idx, frame in enumerate(frames_iter):\n",
    "                if idx % CONFIG['dataset']['frame_step'] == 0:\n",
    "                    resized = tf.image.resize(frame, resolution).numpy()\n",
    "                    frames.append(resized)\n",
    "                if len(frames) >= num_frames:\n",
    "                    break\n",
    "        else:\n",
    "            count_frames_method = getattr(reader, 'count_frames', None)\n",
    "            if callable(count_frames_method):\n",
    "                try:\n",
    "                    total_frames = int(count_frames_method())\n",
    "                except Exception:\n",
    "                    total_frames = num_frames * CONFIG['dataset']['frame_step']\n",
    "            else:\n",
    "                total_frames = num_frames * CONFIG['dataset']['frame_step']\n",
    "            get_data_method = getattr(reader, 'get_data', None)\n",
    "            if not callable(get_data_method):\n",
    "                raise AttributeError('Reader object does not provide a callable get_data method.')\n",
    "            for frame_index in range(total_frames):\n",
    "                try:\n",
    "                    frame = get_data_method(frame_index)\n",
    "                except IndexError:\n",
    "                    break\n",
    "                if frame_index % CONFIG['dataset']['frame_step'] == 0:\n",
    "                    resized = tf.image.resize(frame, resolution).numpy()\n",
    "                    frames.append(resized)\n",
    "                if len(frames) >= num_frames:\n",
    "                    break\n",
    "    if not frames:\n",
    "        raise ValueError(f'No frames decoded from {path}. / 无法从视频解码出帧：{path}')\n",
    "    if len(frames) < num_frames:\n",
    "        # Loop-pad to reach required length / 若帧数不足则循环补帧\n",
    "        repeat = num_frames - len(frames)\n",
    "        frames.extend(frames[:repeat])\n",
    "    clip = np.stack(frames, axis=0)\n",
    "    clip = clip.astype('float32')\n",
    "    clip = (clip / 127.5) - 1.0\n",
    "    clip = np.transpose(clip, (3, 0, 1, 2))  # [C, T, H, W] / 通道顺序调整为 [通道, 时间, 高, 宽]\n",
    "    return clip\n",
    "\n",
    "def load_metadata(metadata_csv: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(metadata_csv)\n",
    "    required_cols = {'video_path', 'text'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f'Metadata CSV must contain columns: {required_cols}')\n",
    "    df['video_path'] = df['video_path'].apply(lambda p: os.path.join(CONFIG['dataset']['video_root'], p))\n",
    "    return df\n",
    "\n",
    "class VideoTextDataset(keras.utils.Sequence):\n",
    "    \"\"\"Keras Sequence for streaming batches of (video, text_embedding) pairs. / 基于 Keras Sequence 的视频-文本批量加载器\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe: pd.DataFrame, batch_size: int, num_frames: int, resolution: Tuple[int, int]):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_frames = num_frames\n",
    "        self.resolution = resolution\n",
    "        self.indices = np.arange(len(self.df))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        videos = []\n",
    "        texts = []\n",
    "        for i in batch_indices:\n",
    "            row = self.df.iloc[i]\n",
    "            clip = load_video_clip(row['video_path'], self.num_frames, self.resolution)\n",
    "            videos.append(clip)\n",
    "            texts.append(row['text'])\n",
    "        video_batch = np.stack(videos, axis=0)  # [B, C, T, H, W] / 批次张量形状\n",
    "        text_embeddings = TEXT_ENCODER(texts)\n",
    "        return video_batch, text_embeddings\n",
    "\n",
    "def create_datasets(metadata_csv: str) -> Tuple[VideoTextDataset, VideoTextDataset]:\n",
    "    df = load_metadata(metadata_csv)\n",
    "    split_idx = int(len(df) * CONFIG['dataset']['train_split'])\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    val_df = df.iloc[split_idx:]\n",
    "    train_dataset = VideoTextDataset(train_df, CONFIG['training']['batch_size_stage1'], CONFIG['dataset']['frames'], CONFIG['dataset']['resolution'])\n",
    "    val_dataset = VideoTextDataset(val_df, CONFIG['training']['batch_size_stage1'], CONFIG['dataset']['frames'], CONFIG['dataset']['resolution'])\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5baf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset diagnostics and visualization / 数据集统计与可视化\n",
    "def dataset_statistics(metadata_csv: str, sample_count: int = 4):\n",
    "    df = load_metadata(metadata_csv)\n",
    "    print(f'Total samples: {len(df)} / 数据总量')\n",
    "    if 'num_frames' in df.columns:\n",
    "        frame_counts = df['num_frames'].to_numpy()\n",
    "        print('Average clip length:', frame_counts.mean(), '/ 平均帧数')\n",
    "        sns.histplot(frame_counts, bins=20)\n",
    "        plt.title('Frame count distribution / 帧数分布')\n",
    "        plt.show()\n",
    "    text_lengths = df['text'].str.split().apply(len).to_numpy()\n",
    "    sns.histplot(text_lengths, bins=20)\n",
    "    plt.title('Text length distribution (tokens) / 文本长度分布（词数）')\n",
    "    plt.show()\n",
    "    sample_df = df.sample(n=min(sample_count, len(df)), random_state=SEED)\n",
    "    fig, axes = plt.subplots(1, sample_df.shape[0], figsize=(4 * sample_df.shape[0], 4))\n",
    "    axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    for ax, (_, row) in zip(axes, sample_df.iterrows()):\n",
    "        clip = load_video_clip(row['video_path'], CONFIG['dataset']['frames'], CONFIG['dataset']['resolution'])\n",
    "        # Display the first frame for a quick glimpse / 可视化第一帧以快速预览\n",
    "        frame = clip[:, 0]  # [C, H, W] / 张量形状为 [通道, 高, 宽]\n",
    "        frame = np.transpose(frame, (1, 2, 0))\n",
    "        frame = ((frame + 1.0) * 127.5).astype('uint8')\n",
    "        ax.imshow(frame)\n",
    "        ax.set_title(row['text'][:60] + '...')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return df\n",
    "\n",
    "# Example usage (uncomment after setting metadata path) / 示例：配置路径后取消注释\n",
    "# dataset_statistics(CONFIG['dataset']['metadata_csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Video VAE (optional but recommended) / 视频潜空间 VAE（可选但推荐）\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_video_vae(input_shape: Tuple[int, int, int, int], latent_dim: int):\n",
    "    channels, frames, height, width = input_shape\n",
    "    encoder_inputs = keras.Input(shape=(channels, frames, height, width), name='encoder_input')\n",
    "    x = layers.Permute((2, 3, 4, 1))(encoder_inputs)  # [T, H, W, C] / 维度换位方便 3D 卷积\n",
    "    x = layers.Conv3D(64, 3, strides=(1, 2, 2), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv3D(128, 3, strides=(2, 2, 2), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv3D(256, 3, strides=(2, 2, 2), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='video_encoder')\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = layers.Dense((frames // 4) * (height // 8) * (width // 8) * 256)(latent_inputs)\n",
    "    x = layers.Reshape((frames // 4, height // 8, width // 8, 256))(x)\n",
    "    x = layers.Conv3DTranspose(256, 3, strides=(2, 2, 2), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv3DTranspose(128, 3, strides=(2, 2, 2), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv3DTranspose(64, 3, strides=(1, 2, 2), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    decoder_outputs = layers.Conv3DTranspose(channels, 3, activation='tanh', padding='same', name='decoder_output')(x)\n",
    "    decoder_outputs = layers.Permute((4, 1, 2, 3))(decoder_outputs)\n",
    "\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name='video_decoder')\n",
    "\n",
    "    class VideoVAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name='total_loss')\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(name='reconstruction_loss')\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name='kl_loss')\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var, z = self.encoder(inputs)\n",
    "            reconstruction = self.decoder(z)\n",
    "            return reconstruction\n",
    "\n",
    "        def train_step(self, data):\n",
    "            videos, _ = data\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(videos, training=True)\n",
    "                reconstruction = self.decoder(z, training=True)\n",
    "                reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.square(videos - reconstruction), axis=[1, 2, 3, 4]))\n",
    "                kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))\n",
    "                total_loss = reconstruction_loss + kl_loss\n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            return {\n",
    "                'loss': self.total_loss_tracker.result(),\n",
    "                'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "                'kl_loss': self.kl_loss_tracker.result(),\n",
    "            }\n",
    "\n",
    "    vae = VideoVAE(encoder, decoder, name='video_vae')\n",
    "    return vae\n",
    "\n",
    "VAE_MODEL = build_video_vae((CONFIG['model']['channels'], CONFIG['dataset']['frames'], *CONFIG['dataset']['resolution']), CONFIG['model']['vae_latent_dim'])\n",
    "vae_optimizer: Any = keras.optimizers.Adam(learning_rate=float(CONFIG['training']['learning_rate_vae']))\n",
    "VAE_MODEL.compile(optimizer=vae_optimizer)\n",
    "VAE_MODEL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal attention and UNet building blocks / 时间注意力与 UNet 组件\n",
    "def sinusoidal_time_embedding(timesteps: tf.Tensor, dim: int) -> tf.Tensor:\n",
    "    timesteps = tf.convert_to_tensor(timesteps)\n",
    "    half_dim = dim // 2\n",
    "    batch_shape = tf.shape(timesteps)\n",
    "    batch_size = tf.gather(batch_shape, 0)\n",
    "    if half_dim == 0:\n",
    "        return tf.zeros((batch_size, dim), dtype=tf.float32)\n",
    "    denominator = tf.cast(tf.maximum(half_dim - 1, 1), tf.float32)\n",
    "    log_term = tf.math.log(tf.constant(10000.0, dtype=tf.float32))\n",
    "    exponent = -log_term / denominator\n",
    "    frequencies = tf.exp(tf.range(half_dim, dtype=tf.float32) * exponent)\n",
    "    angles = tf.cast(tf.reshape(timesteps, (-1, 1)), tf.float32) * tf.reshape(frequencies, (1, -1))\n",
    "    emb = tf.concat([tf.sin(angles), tf.cos(angles)], axis=1)\n",
    "    if dim % 2 == 1:\n",
    "        emb = tf.pad(emb, [[0, 0], [0, 1]])\n",
    "    emb = tf.cast(emb, dtype=tf.float32)\n",
    "    return tf.ensure_shape(emb, [None, dim])\n",
    "\n",
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, channels: int, time_emb_dim: int, use_attention: bool = False, num_heads: int = 4, name: str = 'res_block'):\n",
    "        super().__init__(name=name)\n",
    "        self.channels = channels\n",
    "        self.time_mlp = keras.Sequential([layers.Dense(channels, activation='gelu'), layers.Dense(channels)])\n",
    "        self.conv1 = layers.Conv3D(channels, 3, padding='same')\n",
    "        self.conv2 = layers.Conv3D(channels, 3, padding='same')\n",
    "        self.norm1 = layers.LayerNormalization(axis=-1)\n",
    "        self.norm2 = layers.LayerNormalization(axis=-1)\n",
    "        self.act = layers.Activation('gelu')\n",
    "        self.use_attention = use_attention\n",
    "        if use_attention:\n",
    "            self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=max(channels // num_heads, 1))\n",
    "            self.attn_norm = layers.LayerNormalization(axis=-1)\n",
    "        self.proj = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if input_shape[-1] != self.channels:\n",
    "            self.proj = layers.Conv3D(self.channels, 1, padding='same')\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, time_emb: tf.Tensor):\n",
    "        h = self.norm1(inputs)\n",
    "        h = self.act(h)\n",
    "        h = self.conv1(h)\n",
    "        time_emb_proj = self.time_mlp(time_emb)\n",
    "        time_emb_proj = tf.reshape(time_emb_proj, (-1, 1, 1, 1, self.channels))\n",
    "        h = h + time_emb_proj\n",
    "        h = self.norm2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.conv2(h)\n",
    "        if self.use_attention:\n",
    "            shape_tensor = tf.shape(h)\n",
    "            batch, time_steps, height, width, channels = tf.unstack(shape_tensor)\n",
    "            flattened = tf.reshape(h, (batch, time_steps, height * width, channels))\n",
    "            attn_out = self.attn(flattened, flattened)\n",
    "            attn_out = self.attn_norm(attn_out)\n",
    "            attn_out = tf.reshape(attn_out, (batch, time_steps, height, width, channels))\n",
    "            h = h + attn_out\n",
    "        residual = inputs if self.proj is None else self.proj(inputs)\n",
    "        return residual + h\n",
    "\n",
    "class Downsample(layers.Layer):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = layers.Conv3D(channels, 3, strides=(1, 2, 2), padding='same')\n",
    "    def call(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample(layers.Layer):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = layers.Conv3DTranspose(channels, 3, strides=(1, 2, 2), padding='same')\n",
    "    def call(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd33dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video UNet conditioned on text embeddings / 文本条件视频 UNet\n",
    "class VideoUNet(keras.Model):\n",
    "    def __init__(self, base_channels: int, channel_mults: List[int], attention_resolutions: List[int], num_heads: int, time_emb_dim: int, text_emb_dim: int):\n",
    "        super().__init__(name='video_unet')\n",
    "        self.time_mlp = keras.Sequential([\n",
    "            layers.Dense(time_emb_dim, activation='gelu'),\n",
    "            layers.Dense(time_emb_dim)\n",
    "        ])\n",
    "        self.text_proj = layers.Dense(time_emb_dim)\n",
    "        self.input_conv = layers.Conv3D(base_channels, 3, padding='same')\n",
    "        self.downs = []\n",
    "        self.ups = []\n",
    "        in_channels = base_channels\n",
    "        resolution = CONFIG['dataset']['resolution'][0]\n",
    "        for mult in channel_mults:\n",
    "            out_channels = base_channels * mult\n",
    "            use_attention = resolution in attention_resolutions\n",
    "            self.downs.append([\n",
    "                ResidualBlock(out_channels, time_emb_dim, use_attention=use_attention, num_heads=num_heads),\n",
    "                ResidualBlock(out_channels, time_emb_dim, use_attention=use_attention, num_heads=num_heads),\n",
    "                Downsample(out_channels)\n",
    "            ])\n",
    "            in_channels = out_channels\n",
    "            resolution //= 2\n",
    "        self.mid_block1 = ResidualBlock(in_channels, time_emb_dim, use_attention=True, num_heads=num_heads)\n",
    "        self.mid_block2 = ResidualBlock(in_channels, time_emb_dim, use_attention=False, num_heads=num_heads)\n",
    "        for mult in reversed(channel_mults):\n",
    "            out_channels = base_channels * mult\n",
    "            use_attention = resolution in attention_resolutions\n",
    "            self.ups.append([\n",
    "                Upsample(out_channels),\n",
    "                ResidualBlock(out_channels, time_emb_dim, use_attention=use_attention, num_heads=num_heads),\n",
    "                ResidualBlock(out_channels, time_emb_dim, use_attention=use_attention, num_heads=num_heads)\n",
    "            ])\n",
    "            resolution *= 2\n",
    "        self.output_conv = layers.Conv3D(CONFIG['model']['channels'], 3, padding='same')\n",
    "\n",
    "    def call(self, x, timesteps, text_embeddings, training=False):\n",
    "        t_emb = sinusoidal_time_embedding(timesteps, CONFIG['model']['max_time_embeddings'])\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "        txt_emb = self.text_proj(text_embeddings)\n",
    "        combined_emb = tf.nn.gelu(t_emb + txt_emb)\n",
    "        h = self.input_conv(x)\n",
    "        skips = []\n",
    "        for res_blocks in self.downs:\n",
    "            for block in res_blocks[:-1]:\n",
    "                h = block(h, combined_emb)\n",
    "                skips.append(h)\n",
    "            h = res_blocks[-1](h)\n",
    "        h = self.mid_block1(h, combined_emb)\n",
    "        h = self.mid_block2(h, combined_emb)\n",
    "        for res_blocks in self.ups:\n",
    "            upsample, block1, block2 = res_blocks\n",
    "            h = upsample(h)\n",
    "            skip = skips.pop()\n",
    "            h = tf.concat([h, skip], axis=-1)\n",
    "            h = block1(h, combined_emb)\n",
    "            skip = skips.pop()\n",
    "            h = tf.concat([h, skip], axis=-1)\n",
    "            h = block2(h, combined_emb)\n",
    "        output = self.output_conv(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7537a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian diffusion schedule and model wrapper / 高斯扩散调度与模型封装\n",
    "class GaussianDiffusion:\n",
    "    def __init__(self, timesteps: int, beta_schedule: str = 'cosine'):\n",
    "        self.timesteps = timesteps\n",
    "        if beta_schedule == 'linear':\n",
    "            betas = np.linspace(1e-4, 0.02, timesteps, dtype=np.float32)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            steps = np.arange(timesteps + 1, dtype=np.float64) / timesteps\n",
    "            alphas_cumprod = np.cos((steps + 0.008) / 1.008 * math.pi / 2) ** 2\n",
    "            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "            betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "            betas = np.clip(betas, 1e-5, 0.999)\n",
    "        else:\n",
    "            raise ValueError('Unsupported beta schedule')\n",
    "        self.betas = tf.constant(betas, dtype=tf.float32)\n",
    "        ones = tf.ones_like(self.betas)\n",
    "        self.alphas = ones - self.betas\n",
    "        self.alphas_cumprod = tf.math.cumprod(self.alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = tf.concat([tf.constant([1.0], dtype=tf.float32), self.alphas_cumprod[:-1]], axis=0)\n",
    "        self.sqrt_alphas_cumprod = tf.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = tf.sqrt(tf.ones_like(self.alphas_cumprod) - self.alphas_cumprod)\n",
    "        self.posterior_variance = self.betas * (tf.ones_like(self.alphas_cumprod_prev) - self.alphas_cumprod_prev) / (tf.ones_like(self.alphas_cumprod) - self.alphas_cumprod)\n",
    "\n",
    "    def q_sample(self, x_start: tf.Tensor, t: tf.Tensor, noise: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
    "        if noise is None:\n",
    "            noise = tf.random.normal(tf.shape(x_start))\n",
    "        sqrt_alphas_cumprod_t = tf.reshape(tf.gather(self.sqrt_alphas_cumprod, t), (-1, 1, 1, 1, 1))\n",
    "        sqrt_one_minus = tf.reshape(tf.gather(self.sqrt_one_minus_alphas_cumprod, t), (-1, 1, 1, 1, 1))\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus * noise\n",
    "\n",
    "    def p_sample(self, denoise_fn, x, t, text_embeddings, guidance_scale: float = 1.0):\n",
    "        beta_t = tf.reshape(tf.gather(self.betas, t), (-1, 1, 1, 1, 1))\n",
    "        alpha_t = tf.reshape(tf.gather(self.alphas, t), (-1, 1, 1, 1, 1))\n",
    "        sqrt_one_minus = tf.reshape(tf.gather(self.sqrt_one_minus_alphas_cumprod, t), (-1, 1, 1, 1, 1))\n",
    "        sqrt_recip_alpha = tf.math.rsqrt(alpha_t)\n",
    "        z = tf.random.normal(tf.shape(x))\n",
    "        eps = denoise_fn(x, t, text_embeddings)\n",
    "        if guidance_scale != 1.0:\n",
    "            eps = eps * guidance_scale\n",
    "        x0_pred = (x - sqrt_one_minus * eps) * sqrt_recip_alpha\n",
    "        dir_xt = tf.sqrt(tf.ones_like(beta_t) - beta_t) * x\n",
    "        noise_term = tf.sqrt(beta_t) * z\n",
    "        return dir_xt + noise_term, x0_pred\n",
    "\n",
    "DIFFUSION = GaussianDiffusion(CONFIG['model']['diffusion_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion model wrapper integrating UNet and training logic / 融合 UNet 与训练逻辑的扩散模型\n",
    "class VideoDiffusionModel(keras.Model):\n",
    "    def __init__(self, unet: VideoUNet, diffusion: GaussianDiffusion):\n",
    "        super().__init__(name='video_diffusion')\n",
    "        self.unet = unet\n",
    "        self.diffusion = diffusion\n",
    "        self.loss_tracker = keras.metrics.Mean(name='diffusion_loss')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        videos, text_embeddings = inputs\n",
    "        video_shape = tf.shape(videos, out_type=tf.int32)\n",
    "        batch_size = tf.gather(video_shape, 0)\n",
    "        timesteps = tf.random.uniform(\n",
    "            shape=tf.reshape(batch_size, (1,)),\n",
    "            minval=0,\n",
    "            maxval=self.diffusion.timesteps,\n",
    "            dtype=tf.int32,\n",
    "        )\n",
    "        timesteps = tf.reshape(timesteps, (-1,))\n",
    "        noise = tf.random.normal(tf.shape(videos))\n",
    "        noisy_videos = self.diffusion.q_sample(videos, timesteps, noise)\n",
    "        pred_noise = self.unet(noisy_videos, timesteps, text_embeddings, training=training)\n",
    "        loss = tf.reduce_mean(tf.square(noise - pred_noise))\n",
    "        if training:\n",
    "            self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        videos, text_embeddings = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self(videos, text_embeddings, training=True)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        clipped_gradients = []\n",
    "        for grad in gradients:\n",
    "            if grad is None:\n",
    "                clipped_gradients.append(None)\n",
    "            elif CONFIG['training']['gradient_clip_norm'] is not None:\n",
    "                clipped_gradients.append(tf.clip_by_norm(grad, CONFIG['training']['gradient_clip_norm']))\n",
    "            else:\n",
    "                clipped_gradients.append(grad)\n",
    "        grads_and_vars = [(grad, var) for grad, var in zip(clipped_gradients, self.trainable_variables) if grad is not None]\n",
    "        self.optimizer.apply_gradients(grads_and_vars)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {'loss': self.loss_tracker.result()}\n",
    "\n",
    "    def sample(self, text_embeddings: tf.Tensor, num_frames: int, resolution: Tuple[int, int], num_steps: int, guidance_scale: float = 1.0):\n",
    "        text_shape = tf.shape(text_embeddings, out_type=tf.int32)\n",
    "        batch_size = tf.gather(text_shape, 0)\n",
    "        batch_shape = tf.reshape(batch_size, (1,))\n",
    "        shape_tail = tf.constant([CONFIG['model']['channels'], num_frames, resolution[0], resolution[1]], dtype=tf.int32)\n",
    "        sample_shape = tf.concat([batch_shape, shape_tail], axis=0)\n",
    "        x = tf.random.normal(sample_shape)\n",
    "        latest_x0 = x\n",
    "        dims = tf.reshape(batch_size, (1,))\n",
    "        for step in reversed(range(num_steps)):\n",
    "            timestep_tensor = tf.fill(dims, tf.cast(step, tf.int32))\n",
    "            x, latest_x0 = self.diffusion.p_sample(self.unet, x, timestep_tensor, text_embeddings, guidance_scale)\n",
    "        return x, latest_x0\n",
    "\n",
    "UNET = VideoUNet(\n",
    "    base_channels=CONFIG['model']['base_channels'],\n",
    "    channel_mults=CONFIG['model']['channel_multipliers'],\n",
    "    attention_resolutions=CONFIG['model']['attention_resolutions'],\n",
    "    num_heads=CONFIG['model']['num_heads'],\n",
    "    time_emb_dim=CONFIG['model']['max_time_embeddings'],\n",
    "    text_emb_dim=TEXT_ENCODER.embedding_dim\n",
    ")\n",
    "diffusion_optimizer: Any = keras.optimizers.AdamW(\n",
    "    learning_rate=float(CONFIG['training']['learning_rate_diffusion']),\n",
    "    weight_decay=float(CONFIG['training']['weight_decay'])\n",
    " )\n",
    "DIFFUSION_MODEL = VideoDiffusionModel(UNET, DIFFUSION)\n",
    "DIFFUSION_MODEL.compile(optimizer=diffusion_optimizer)\n",
    "DIFFUSION_MODEL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_lr, warmup_steps, total_steps):\n",
    "        super().__init__()\n",
    "        self.base_lr = tf.convert_to_tensor(base_lr, dtype=tf.float32)\n",
    "        self.warmup_steps = tf.convert_to_tensor(warmup_steps, dtype=tf.float32)\n",
    "        self.total_steps = tf.convert_to_tensor(total_steps, dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.convert_to_tensor(step, dtype=tf.float32)\n",
    "        zero_scalar = tf.constant(0.0, dtype=tf.float32)\n",
    "        warmup_positive = tf.math.greater(self.warmup_steps, zero_scalar)\n",
    "        warmup_divisor = tf.where(warmup_positive, self.warmup_steps, tf.ones_like(self.warmup_steps))\n",
    "        warmup_ratio = step / warmup_divisor\n",
    "        warmup_ratio = tf.clip_by_value(warmup_ratio, 0.0, 1.0)\n",
    "        warmup_lr = self.base_lr * warmup_ratio\n",
    "        pi_const = tf.constant(math.pi, dtype=tf.float32)\n",
    "        total_positive = tf.math.greater(self.total_steps, zero_scalar)\n",
    "        total_steps_safe = tf.where(total_positive, self.total_steps, tf.ones_like(self.total_steps))\n",
    "        cosine_argument = pi_const * step / total_steps_safe\n",
    "        half = tf.constant(0.5, dtype=tf.float32)\n",
    "        one = tf.constant(1.0, dtype=tf.float32)\n",
    "        cosine_decay = half * (one + tf.cos(cosine_argument))\n",
    "        cosine_lr = self.base_lr * cosine_decay\n",
    "        use_warmup = tf.math.less(step, self.warmup_steps)\n",
    "        return tf.where(use_warmup, warmup_lr, cosine_lr)\n",
    "\n",
    "class ExponentialMovingAverage:\n",
    "    def __init__(self, model: keras.Model, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow_vars = [tf.Variable(w, trainable=False) for w in model.weights]\n",
    "        self.model = model\n",
    "        self.backup: List[tf.Tensor] = []\n",
    "\n",
    "    def update(self):\n",
    "        for shadow, weight in zip(self.shadow_vars, self.model.weights):\n",
    "            decay_tensor = tf.cast(tf.convert_to_tensor(self.decay, dtype=shadow.dtype), shadow.dtype)\n",
    "            one_minus = tf.cast(tf.convert_to_tensor(1.0 - self.decay, dtype=shadow.dtype), shadow.dtype)\n",
    "            shadow_tensor = tf.cast(tf.convert_to_tensor(shadow), shadow.dtype)\n",
    "            weight_tensor = tf.cast(tf.convert_to_tensor(weight), shadow.dtype)\n",
    "            updated_shadow = tf.add(tf.multiply(shadow_tensor, decay_tensor), tf.multiply(weight_tensor, one_minus))\n",
    "            shadow.assign(updated_shadow)\n",
    "\n",
    "    def apply_ema_weights(self):\n",
    "        backup_tensors: List[tf.Tensor] = []\n",
    "        for weight in self.model.weights:\n",
    "            weight_tensor = tf.cast(tf.convert_to_tensor(weight), weight.dtype)\n",
    "            backup_tensors.append(tf.identity(weight_tensor))\n",
    "        self.backup = backup_tensors\n",
    "        for weight, shadow in zip(self.model.weights, self.shadow_vars):\n",
    "            weight.assign(tf.cast(shadow, weight.dtype))\n",
    "\n",
    "    def restore_weights(self):\n",
    "        for weight, backup_weight in zip(self.model.weights, self.backup):\n",
    "            weight.assign(tf.cast(backup_weight, weight.dtype))\n",
    "\n",
    "def setup_tensorboard(log_dir: str, name: str):\n",
    "    log_path = os.path.join(log_dir, name)\n",
    "    pathlib.Path(log_path).mkdir(parents=True, exist_ok=True)\n",
    "    writer = tf.summary.create_file_writer(log_path)\n",
    "    return writer\n",
    "\n",
    "def save_checkpoint(model: keras.Model, optimizer: keras.optimizers.Optimizer, epoch: int, prefix: str):\n",
    "    checkpoint_path = os.path.join(CONFIG['output_dir'], f'{prefix}_epoch_{epoch}.weights.h5')\n",
    "    model.save_weights(checkpoint_path)\n",
    "    print(f'Saved checkpoint: {checkpoint_path} / 已保存检查点')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce284b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage-wise training functions / 分阶段训练流程\n",
    "def train_vae(train_dataset: VideoTextDataset, val_dataset: VideoTextDataset):\n",
    "    writer = setup_tensorboard(CONFIG['log_dir'], 'vae')\n",
    "    for epoch in range(CONFIG['training']['vae_epochs']):\n",
    "        print(f\"VAE Epoch {epoch + 1}/{CONFIG['training']['vae_epochs']} / VAE 训练轮次\")\n",
    "        for batch in tqdm(train_dataset, desc='VAE Train / VAE 训练'):\n",
    "            videos, _ = batch\n",
    "            VAE_MODEL.train_on_batch(videos, videos)\n",
    "        val_losses = []\n",
    "        for batch in val_dataset:\n",
    "            videos, _ = batch\n",
    "            val_result = VAE_MODEL.evaluate(videos, videos, verbose='auto')\n",
    "            val_loss = val_result['loss'] if isinstance(val_result, dict) else val_result\n",
    "            val_losses.append(float(val_loss))\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('val_loss', np.mean(val_losses), step=epoch)\n",
    "    return VAE_MODEL\n",
    "\n",
    "def train_diffusion(train_dataset: VideoTextDataset, val_dataset: VideoTextDataset, stage: str = 'stage1'):\n",
    "    num_epochs = CONFIG['training']['diffusion_epochs_stage1'] if stage == 'stage1' else CONFIG['training']['diffusion_epochs_stage2']\n",
    "    ema = ExponentialMovingAverage(DIFFUSION_MODEL, CONFIG['training']['ema_decay'])\n",
    "    writer = setup_tensorboard(CONFIG['log_dir'], f'diffusion_{stage}')\n",
    "    global_step = 0\n",
    "    total_steps = num_epochs * max(len(train_dataset), 1)\n",
    "    lr_schedule = WarmupCosineSchedule(\n",
    "        base_lr=CONFIG['training']['learning_rate_diffusion'],\n",
    "        warmup_steps=CONFIG['training']['warmup_steps'],\n",
    "        total_steps=total_steps\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('learning_rate', lr_schedule(global_step), step=global_step)\n",
    "        print(f'Diffusion {stage} Epoch {epoch + 1}/{num_epochs} / 扩散阶段 {stage} 轮次')\n",
    "        for videos, text_embeddings in tqdm(train_dataset, desc='Diffusion Train / 扩散模型训练'):\n",
    "            DIFFUSION_MODEL.optimizer.learning_rate = lr_schedule(global_step)\n",
    "            train_result = DIFFUSION_MODEL.train_on_batch(videos, text_embeddings)\n",
    "            train_loss = train_result['loss'] if isinstance(train_result, dict) else train_result\n",
    "            ema.update()\n",
    "            if global_step % 50 == 0:\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar('train_loss', tf.convert_to_tensor(train_loss), step=global_step)\n",
    "            global_step += 1\n",
    "        val_losses = []\n",
    "        for videos, text_embeddings in val_dataset:\n",
    "            val_result = DIFFUSION_MODEL.evaluate(videos, text_embeddings, verbose='auto')\n",
    "            val_loss = val_result['loss'] if isinstance(val_result, dict) else val_result\n",
    "            val_losses.append(float(val_loss))\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('val_loss', np.mean(val_losses), step=global_step)\n",
    "        if (epoch + 1) % CONFIG['training']['checkpoint_interval'] == 0:\n",
    "            save_checkpoint(DIFFUSION_MODEL, DIFFUSION_MODEL.optimizer, epoch + 1, stage)\n",
    "    return DIFFUSION_MODEL, ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb77c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation utilities / 视频生成工具\n",
    "def latent_to_video(latents: tf.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert latent or pixel-space tensors to uint8 video arrays. / 将潜变量或像素空间张量转为 uint8 视频帧\"\"\"\n",
    "    tensor = tf.convert_to_tensor(latents)\n",
    "    if len(tensor.shape) == 2 and VAE_MODEL is not None:\n",
    "        reconstruction = VAE_MODEL.decoder(tensor, training=False)\n",
    "    else:\n",
    "        reconstruction = tensor\n",
    "    video = tf.transpose(reconstruction, perm=[0, 2, 3, 4, 1])\n",
    "    video = tf.clip_by_value((video + 1.0) * 127.5, 0.0, 255.0)\n",
    "    return tf.cast(video, tf.uint8).numpy()\n",
    "\n",
    "def save_video(frames: np.ndarray, path: str, fps: int = 8, fmt: str = 'mp4'):\n",
    "    frames_list = list(frames)\n",
    "    if fmt == 'gif':\n",
    "        imageio.mimsave(path, frames_list, fps=fps)\n",
    "    else:\n",
    "        clip = mpy.ImageSequenceClip(frames_list, fps=fps)\n",
    "        clip.write_videofile(path, codec='libx264', audio=False, verbose=False, logger=None)\n",
    "\n",
    "def display_video(frames: np.ndarray, fps: int = 8):\n",
    "    clip = mpy.ImageSequenceClip(list(frames), fps=fps)\n",
    "    video_html = clip.to_html5_video()  # type: ignore[attr-defined]\n",
    "    display(HTML(video_html))\n",
    "\n",
    "def generate_video(prompt: str, num_frames: int = 16, resolution: Tuple[int, int] = (64, 64), batch_size: int = 1, num_steps: Optional[int] = None, guidance_scale: Optional[float] = None, save_path: Optional[str] = None) -> np.ndarray:\n",
    "    steps = num_steps if num_steps is not None else CONFIG['generation']['num_inference_steps']\n",
    "    guidance = guidance_scale if guidance_scale is not None else CONFIG['generation']['guidance_scale']\n",
    "    if len(resolution) != 2:\n",
    "        raise ValueError('Resolution must be a (height, width) tuple of length 2.')\n",
    "    height, width = int(resolution[0]), int(resolution[1])\n",
    "    text_embeddings = TEXT_ENCODER([prompt] * batch_size)\n",
    "    _, predictions = DIFFUSION_MODEL.sample(text_embeddings, num_frames, (height, width), steps, guidance)\n",
    "    videos = latent_to_video(predictions)\n",
    "    for idx, video in enumerate(videos):\n",
    "        if save_path and batch_size == 1:\n",
    "            target_path = save_path\n",
    "        else:\n",
    "            base_dir = os.path.dirname(save_path) if save_path else CONFIG['output_dir']\n",
    "            pathlib.Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "            target_path = os.path.join(base_dir, f'generated_{idx}.mp4')\n",
    "        save_video(video, target_path, fmt='mp4')\n",
    "        print(f'Saved video to {target_path} / 视频已保存')\n",
    "        display_video(video)\n",
    "    return videos\n",
    "\n",
    "# Example usage (after training) / 训练完成后的示例用法\n",
    "# generate_video('A tranquil lake surrounded by snowy mountains during sunrise.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: plotting losses, qualitative samples, FVD placeholder / 评估：损失曲线、案例展示与 FVD 占位\n",
    "def plot_loss_curves(log_dir: str, run_name: str):\n",
    "    event_path = os.path.join(log_dir, run_name)\n",
    "    print(f'Use TensorBoard to inspect logs: tensorboard --logdir {event_path} / 使用 TensorBoard 查看日志')\n",
    "\n",
    "def compare_prompts(prompts: List[str]):\n",
    "    fig, axes = plt.subplots(len(prompts), 1, figsize=(6, 4 * len(prompts)))\n",
    "    axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    for ax, prompt in zip(axes, prompts):\n",
    "        videos = generate_video(prompt, save_path=os.path.join(CONFIG['output_dir'], f\"{prompt[:20].replace(' ', '_')}.mp4\"))\n",
    "        ax.imshow(videos[0][0])\n",
    "        ax.set_title(prompt)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_fvd_placeholder(real_videos: np.ndarray, generated_videos: np.ndarray) -> float:\n",
    "    # TODO: Integrate official FVD computation (requires I3D features). / 待办：集成官方 FVD 计算（需 I3D 特征）\n",
    "    # Placeholder returns NaN and reminds to implement. / 目前返回 NaN 作为提醒\n",
    "    return float('nan')\n",
    "\n",
    "def ablation_experiment(prompts: List[str], frame_counts: List[int], resolutions: List[Tuple[int, int]], use_attention: List[bool]):\n",
    "    results = []\n",
    "    for prompt in prompts:\n",
    "        for frames in frame_counts:\n",
    "            for res in resolutions:\n",
    "                for attention in use_attention:\n",
    "                    if not attention:\n",
    "                        # Temporarily disable attention by adjusting UNET parameters / 临时关闭注意力以做对比实验\n",
    "                        original_attention = CONFIG['model']['attention_resolutions']\n",
    "                        CONFIG['model']['attention_resolutions'] = []\n",
    "                    videos = generate_video(prompt, num_frames=frames, resolution=res)\n",
    "                    results.append({'prompt': prompt, 'frames': frames, 'resolution': res, 'attention': attention})\n",
    "                    if not attention:\n",
    "                        CONFIG['model']['attention_resolutions'] = original_attention\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8129732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Gradio interface for interactive demos / 可选的 Gradio 交互式演示界面\n",
    "def setup_gradio_interface():\n",
    "    if not GRADIO_AVAILABLE:\n",
    "        raise ImportError('Gradio not installed. Run `pip install gradio`. / 未安装 Gradio，请先运行 pip install gradio')\n",
    "\n",
    "    def infer(prompt, frames, resolution):\n",
    "        res_values = tuple(map(int, resolution.split('x')))\n",
    "        if len(res_values) != 2:\n",
    "            raise ValueError('Resolution dropdown must provide values like 64x64. / Gradio 分辨率格式需为 如 64x64')\n",
    "        res_tuple: Tuple[int, int] = (res_values[0], res_values[1])\n",
    "        videos = generate_video(prompt, num_frames=int(frames), resolution=res_tuple)\n",
    "        temp_path = os.path.join(CONFIG['output_dir'], 'gradio_preview.mp4')\n",
    "        save_video(videos[0], temp_path)\n",
    "        return temp_path\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown('# Text-to-Video Diffusion Demo / 文本生成视频实时演示')\n",
    "        prompt = gr.Textbox(label='Prompt / 文本提示', value='A paper boat floating down a rainy street at night.')\n",
    "        frames = gr.Slider(8, 32, value=16, step=1, label='Frames / 帧数')\n",
    "        resolution = gr.Dropdown(['64x64', '96x96', '128x128'], value='64x64', label='Resolution / 分辨率')\n",
    "        btn = gr.Button('Generate / 生成')\n",
    "        output = gr.Video(label='Generated Video / 生成视频')\n",
    "        btn.click(fn=infer, inputs=[prompt, frames, resolution], outputs=output)\n",
    "    return demo\n",
    "\n",
    "# To launch: demo = setup_gradio_interface(); demo.launch(share=False) / 启动方法如上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2video",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
